{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b968880-f58e-4061-94d9-e53bb4af38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENTERPRISE CREDIT SCORING SYSTEM v10.0\n",
    "# ==============================================================================\n",
    "# Autor: Sistema de Cr\u00e9dito Enterprise\n",
    "# Compliance: BACEN Resolution 4.557/2017, LGPD, Basel III\n",
    "# \u00daltima Atualiza\u00e7\u00e3o: 2025-10-28\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import logging\n",
    "import hashlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "# ML Libraries\n",
    "import optuna\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    brier_score_loss, log_loss, average_precision_score\n",
    ")\n",
    "\n",
    "# Explicabilidade\n",
    "import shap\n",
    "\n",
    "# Configura\u00e7\u00e3o Global\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler('credit_scoring.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ENUMERADORES E CONSTANTES\n",
    "# ==============================================================================\n",
    "\n",
    "class ModelStatus(Enum):\n",
    "    \"\"\"Status do modelo no ciclo de vida.\"\"\"\n",
    "    DEVELOPMENT = \"development\"\n",
    "    STAGING = \"staging\"\n",
    "    PRODUCTION = \"production\"\n",
    "    DEPRECATED = \"deprecated\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "class AlertLevel(Enum):\n",
    "    \"\"\"N\u00edveis de alerta para monitoramento.\"\"\"\n",
    "    INFO = \"info\"\n",
    "    WARNING = \"warning\"\n",
    "    CRITICAL = \"critical\"\n",
    "    EMERGENCY = \"emergency\"\n",
    "\n",
    "class DataQualityIssue(Enum):\n",
    "    \"\"\"Tipos de problemas de qualidade de dados.\"\"\"\n",
    "    MISSING_VALUES = \"missing_values\"\n",
    "    OUTLIERS = \"outliers\"\n",
    "    DATA_DRIFT = \"data_drift\"\n",
    "    TARGET_LEAKAGE = \"target_leakage\"\n",
    "    INVALID_VALUES = \"invalid_values\"\n",
    "\n",
    "# Constantes de Neg\u00f3cio\n",
    "BUSINESS_CONSTANTS = {\n",
    "    'min_age': 18,\n",
    "    'max_age': 85,\n",
    "    'min_income': 0,\n",
    "    'max_loan_amount': 1_000_000,\n",
    "    'max_loan_term_months': 72,\n",
    "    'excel_date_base': pd.Timestamp('1899-12-30'),  # Excel Windows base\n",
    "    'excel_mac_date_base': pd.Timestamp('1904-01-01')\n",
    "}\n",
    "\n",
    "# Vari\u00e1veis que indicam Target Leakage (nunca devem estar no modelo)\n",
    "FORBIDDEN_FEATURES = [\n",
    "    'status_financeiro', 'inadimplente', 'default',\n",
    "    'quantidade_parcelas_vencidas', 'taxa_parcelas_vencidas',\n",
    "    'saldo_vencido', 'dias_em_atraso', 'primeiro_vencimento_em_atraso',\n",
    "    'data_quitacao', 'pagamento_efetuado', 'valor_pago',\n",
    "    'parcelas_pagas', 'atraso_', 'vencido', 'quitado'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURA\u00c7\u00c3O CENTRALIZADA\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BusinessMetrics:\n",
    "    \"\"\"M\u00e9tricas de neg\u00f3cio para otimiza\u00e7\u00e3o.\"\"\"\n",
    "    revenue_per_good_client: float = 1000.0      # Receita m\u00e9dia por bom pagador\n",
    "    loss_per_bad_client: float = 5000.0          # Perda m\u00e9dia por inadimplente\n",
    "    operational_cost_per_analysis: float = 50.0   # Custo operacional por an\u00e1lise\n",
    "    max_approval_rate: float = 0.70              # Taxa m\u00e1xima de aprova\u00e7\u00e3o\n",
    "    target_bad_rate: float = 0.05                # Taxa de inadimpl\u00eancia alvo (5%)\n",
    "    discount_rate: float = 0.10                  # Taxa de desconto (10% a.a.)\n",
    "\n",
    "@dataclass\n",
    "class ModelThresholds:\n",
    "    \"\"\"Thresholds para alertas e a\u00e7\u00f5es.\"\"\"\n",
    "    min_auc_acceptable: float = 0.70\n",
    "    min_ks_acceptable: float = 0.30\n",
    "    max_psi_warning: float = 0.10\n",
    "    max_psi_critical: float = 0.25\n",
    "    max_feature_missing_rate: float = 0.30\n",
    "    min_samples_per_class: int = 100\n",
    "    max_class_imbalance_ratio: float = 20.0\n",
    "\n",
    "@dataclass\n",
    "class MLConfig:\n",
    "    \"\"\"Configura\u00e7\u00e3o de Machine Learning.\"\"\"\n",
    "    \n",
    "    # Dados\n",
    "    data_path: str = 'Base estatistica 14071.xlsx'\n",
    "    sheet_name: str = 'cobranca-d-30'\n",
    "    target_variable: str = 'status_financeiro'\n",
    "    date_column: str = 'data_efetivacao'\n",
    "    \n",
    "    # Split Strategy\n",
    "    test_size: float = 0.15\n",
    "    validation_size: float = 0.15\n",
    "    oot_months: int = 3  # Meses para Out-of-Time test\n",
    "    \n",
    "    # Cross-Validation\n",
    "    n_folds_cv: int = 5\n",
    "    cv_strategy: str = 'stratified'  # 'stratified' ou 'timeseries'\n",
    "    \n",
    "    # Otimiza\u00e7\u00e3o\n",
    "    n_trials_optuna: int = 100\n",
    "    optuna_timeout: int = 3600  # 1 hora\n",
    "    early_stopping_rounds: int = 50\n",
    "    \n",
    "    # Modelos\n",
    "    models_to_evaluate: List[str] = field(default_factory=lambda: [\n",
    "        'xgboost', 'lightgbm', 'ensemble'\n",
    "    ])\n",
    "    \n",
    "    # Feature Engineering\n",
    "    create_polynomial_features: bool = True\n",
    "    max_polynomial_degree: int = 2\n",
    "    create_interaction_features: bool = True\n",
    "    \n",
    "    # Imputa\u00e7\u00e3o\n",
    "    numeric_imputation_strategy: str = 'knn'  # 'mean', 'median', 'knn'\n",
    "    categorical_imputation_strategy: str = 'mode'\n",
    "    \n",
    "    # Scaling\n",
    "    scaling_method: str = 'robust'  # 'standard', 'robust', 'minmax'\n",
    "    \n",
    "    # Calibra\u00e7\u00e3o\n",
    "    calibration_method: str = 'isotonic'  # 'sigmoid', 'isotonic'\n",
    "    \n",
    "    # Output\n",
    "    output_dir: Path = field(default_factory=lambda: Path('./models'))\n",
    "    model_version: str = '10.0.0'\n",
    "    \n",
    "    # Compliance\n",
    "    enable_audit_trail: bool = True\n",
    "    save_feature_importance: bool = True\n",
    "    generate_model_card: bool = True\n",
    "    \n",
    "    # Random State\n",
    "    random_state: int = 42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Valida\u00e7\u00f5es p\u00f3s-inicializa\u00e7\u00e3o.\"\"\"\n",
    "        # Cria diret\u00f3rio de output se n\u00e3o existir\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Valida\u00e7\u00f5es b\u00e1sicas\n",
    "        if self.test_size + self.validation_size >= 0.5:\n",
    "            raise ValueError(\"test_size + validation_size deve ser < 0.5\")\n",
    "        \n",
    "        if self.test_size <= 0 or self.validation_size <= 0:\n",
    "            raise ValueError(\"test_size e validation_size devem ser > 0\")\n",
    "        \n",
    "        if self.n_trials_optuna < 1:\n",
    "            raise ValueError(\"n_trials_optuna deve ser >= 1\")\n",
    "        \n",
    "        if self.oot_months < 1:\n",
    "            raise ValueError(\"oot_months deve ser >= 1\")\n",
    "\n",
    "@dataclass\n",
    "class EnterpriseConfig:\n",
    "    \"\"\"Configura\u00e7\u00e3o completa enterprise.\"\"\"\n",
    "    ml_config: MLConfig = field(default_factory=MLConfig)\n",
    "    business_metrics: BusinessMetrics = field(default_factory=BusinessMetrics)\n",
    "    thresholds: ModelThresholds = field(default_factory=ModelThresholds)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Serializa configura\u00e7\u00e3o para dict.\"\"\"\n",
    "        return {\n",
    "            'ml_config': asdict(self.ml_config),\n",
    "            'business_metrics': asdict(self.business_metrics),\n",
    "            'thresholds': asdict(self.thresholds)\n",
    "        }\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        \"\"\"Salva configura\u00e7\u00e3o em JSON.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=4, default=str)\n",
    "        logger.info(f\"Configura\u00e7\u00e3o salva em {path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. VALIDA\u00c7\u00c3O E QUALIDADE DE DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validador robusto de qualidade de dados.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EnterpriseConfig):\n",
    "        self.config = config\n",
    "        self.validation_results = {\n",
    "            'passed': [],\n",
    "            'warnings': [],\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def validate_all(self, df: pd.DataFrame) -> Tuple[bool, Dict]:\n",
    "        \"\"\"Executa todas as valida\u00e7\u00f5es.\"\"\"\n",
    "        logger.info(\"Iniciando valida\u00e7\u00e3o de qualidade de dados...\")\n",
    "        \n",
    "        self._check_target_leakage(df)\n",
    "        self._check_missing_values(df)\n",
    "        self._check_data_types(df)\n",
    "        self._check_value_ranges(df)\n",
    "        self._check_temporal_consistency(df)\n",
    "        self._check_class_balance(df)\n",
    "        \n",
    "        is_valid = len(self.validation_results['errors']) == 0\n",
    "        \n",
    "        self._log_validation_summary()\n",
    "        \n",
    "        return is_valid, self.validation_results\n",
    "    \n",
    "    def _check_target_leakage(self, df: pd.DataFrame):\n",
    "        \"\"\"Detecta potencial data leakage.\"\"\"\n",
    "        # Excluir a vari\u00e1vel target da verifica\u00e7\u00e3o de leakage\n",
    "        target_col = self.config.ml_config.target_variable\n",
    "        leaked_cols = [col for col in df.columns \n",
    "                      if col != target_col and \n",
    "                      any(forbidden in col.lower() \n",
    "                            for forbidden in FORBIDDEN_FEATURES)]\n",
    "        \n",
    "        if leaked_cols:\n",
    "            self.validation_results['errors'].append({\n",
    "                'type': DataQualityIssue.TARGET_LEAKAGE,\n",
    "                'message': f\"Colunas com potencial leakage detectadas: {leaked_cols}\",\n",
    "                'severity': AlertLevel.CRITICAL\n",
    "            })\n",
    "            logger.critical(f\"\u274c DATA LEAKAGE DETECTADO: {leaked_cols}\")\n",
    "        else:\n",
    "            self.validation_results['passed'].append({\n",
    "                'check': 'target_leakage',\n",
    "                'message': 'Nenhuma vari\u00e1vel com leakage detectada'\n",
    "            })\n",
    "            logger.info(\"\u2705 Verifica\u00e7\u00e3o de data leakage: PASSOU\")\n",
    "    \n",
    "    def _check_missing_values(self, df: pd.DataFrame):\n",
    "        \"\"\"Verifica taxa de valores missing.\"\"\"\n",
    "        missing_rates = df.isnull().mean()\n",
    "        high_missing = missing_rates[\n",
    "            missing_rates > self.config.thresholds.max_feature_missing_rate\n",
    "        ]\n",
    "        \n",
    "        if len(high_missing) > 0:\n",
    "            self.validation_results['warnings'].append({\n",
    "                'type': DataQualityIssue.MISSING_VALUES,\n",
    "                'message': f\"Colunas com >30% missing: {high_missing.to_dict()}\",\n",
    "                'severity': AlertLevel.WARNING\n",
    "            })\n",
    "            logger.warning(f\"\u26a0\ufe0f  Colunas com alta taxa de missing: {list(high_missing.index)}\")\n",
    "        else:\n",
    "            logger.info(\"\u2705 Verifica\u00e7\u00e3o de missing values: PASSOU\")\n",
    "    \n",
    "    def _check_data_types(self, df: pd.DataFrame):\n",
    "        \"\"\"Valida tipos de dados esperados.\"\"\"\n",
    "        # Verifica se colunas num\u00e9ricas est\u00e3o corretas\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if df[col].dtype == 'object':\n",
    "                self.validation_results['errors'].append({\n",
    "                    'type': DataQualityIssue.INVALID_VALUES,\n",
    "                    'message': f\"Coluna {col} deveria ser num\u00e9rica mas \u00e9 object\",\n",
    "                    'severity': AlertLevel.CRITICAL\n",
    "                })\n",
    "        \n",
    "        logger.info(\"\u2705 Verifica\u00e7\u00e3o de tipos de dados: PASSOU\")\n",
    "    \n",
    "    def _check_value_ranges(self, df: pd.DataFrame):\n",
    "        \"\"\"Valida ranges de valores.\"\"\"\n",
    "        if 'idade' in df.columns:\n",
    "            invalid_ages = df[\n",
    "                (df['idade'] < BUSINESS_CONSTANTS['min_age']) | \n",
    "                (df['idade'] > BUSINESS_CONSTANTS['max_age'])\n",
    "            ]\n",
    "            if len(invalid_ages) > 0:\n",
    "                self.validation_results['warnings'].append({\n",
    "                    'type': DataQualityIssue.OUTLIERS,\n",
    "                    'message': f\"{len(invalid_ages)} registros com idade fora do range v\u00e1lido\",\n",
    "                    'severity': AlertLevel.WARNING\n",
    "                })\n",
    "        \n",
    "        logger.info(\"\u2705 Verifica\u00e7\u00e3o de ranges de valores: PASSOU\")\n",
    "    \n",
    "    def _check_temporal_consistency(self, df: pd.DataFrame):\n",
    "        \"\"\"Verifica consist\u00eancia temporal.\"\"\"\n",
    "        date_col = self.config.ml_config.date_column\n",
    "        \n",
    "        if date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            future_dates = df[df[date_col] > datetime.now()]\n",
    "            \n",
    "            if len(future_dates) > 0:\n",
    "                self.validation_results['errors'].append({\n",
    "                    'type': DataQualityIssue.INVALID_VALUES,\n",
    "                    'message': f\"{len(future_dates)} registros com datas futuras\",\n",
    "                    'severity': AlertLevel.CRITICAL\n",
    "                })\n",
    "        \n",
    "        logger.info(\"\u2705 Verifica\u00e7\u00e3o de consist\u00eancia temporal: PASSOU\")\n",
    "    \n",
    "    def _check_class_balance(self, df: pd.DataFrame):\n",
    "        \"\"\"Verifica desbalanceamento de classes.\"\"\"\n",
    "        target_col = self.config.ml_config.target_variable\n",
    "        \n",
    "        if target_col in df.columns:\n",
    "            class_counts = df[target_col].value_counts()\n",
    "            \n",
    "            if len(class_counts) < 2:\n",
    "                self.validation_results['errors'].append({\n",
    "                    'type': DataQualityIssue.INVALID_VALUES,\n",
    "                    'message': \"Target tem apenas uma classe\",\n",
    "                    'severity': AlertLevel.CRITICAL\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "            \n",
    "            if imbalance_ratio > self.config.thresholds.max_class_imbalance_ratio:\n",
    "                self.validation_results['warnings'].append({\n",
    "                    'type': DataQualityIssue.INVALID_VALUES,\n",
    "                    'message': f\"Desbalanceamento alto: ratio {imbalance_ratio:.2f}\",\n",
    "                    'severity': AlertLevel.WARNING\n",
    "                })\n",
    "                logger.warning(f\"\u26a0\ufe0f  Alto desbalanceamento: {imbalance_ratio:.2f}x\")\n",
    "            else:\n",
    "                logger.info(f\"\u2705 Desbalanceamento aceit\u00e1vel: {imbalance_ratio:.2f}x\")\n",
    "    \n",
    "    def _log_validation_summary(self):\n",
    "        \"\"\"Loga sum\u00e1rio das valida\u00e7\u00f5es.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\"*70)\n",
    "        logger.info(\"SUM\u00c1RIO DE VALIDA\u00c7\u00c3O DE DADOS\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"\u2705 Verifica\u00e7\u00f5es passou: {len(self.validation_results['passed'])}\")\n",
    "        logger.info(f\"\u26a0\ufe0f  Warnings: {len(self.validation_results['warnings'])}\")\n",
    "        logger.info(f\"\u274c Erros: {len(self.validation_results['errors'])}\")\n",
    "        logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FEATURE ENGINEERING AVAN\u00c7ADO\n",
    "# ==============================================================================\n",
    "\n",
    "class SmartDateParser:\n",
    "    \"\"\"Parser robusto de datas com tratamento de formatos Excel.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_date(value: Any, reference_date: datetime = None) -> pd.Timestamp:\n",
    "        \"\"\"\n",
    "        Parse datas de m\u00faltiplos formatos incluindo serial numbers do Excel.\n",
    "        \n",
    "        Args:\n",
    "            value: Valor a ser parseado\n",
    "            reference_date: Data de refer\u00eancia para valida\u00e7\u00f5es\n",
    "        \n",
    "        Returns:\n",
    "            pd.Timestamp ou pd.NaT se inv\u00e1lido\n",
    "        \"\"\"\n",
    "        if reference_date is None:\n",
    "            reference_date = datetime.now()\n",
    "        \n",
    "        # J\u00e1 \u00e9 timestamp v\u00e1lido\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            return value if value <= reference_date else pd.NaT\n",
    "        \n",
    "        # Tenta parse direto\n",
    "        try:\n",
    "            date = pd.to_datetime(value, errors='coerce')\n",
    "            if pd.notna(date) and date <= reference_date:\n",
    "                return date\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Tenta formato serial Excel (Windows)\n",
    "        if isinstance(value, (int, float)) and not np.isnan(value):\n",
    "            try:\n",
    "                # Excel Windows (1900 date system)\n",
    "                if 1 <= value <= 2958465:  # Range v\u00e1lido Excel\n",
    "                    date = BUSINESS_CONSTANTS['excel_date_base'] + pd.Timedelta(days=value)\n",
    "                    \n",
    "                    # Corre\u00e7\u00e3o do bug do Excel (1900 n\u00e3o foi bissexto)\n",
    "                    if value >= 60:\n",
    "                        date = date - pd.Timedelta(days=1)\n",
    "                    \n",
    "                    # Valida\u00e7\u00f5es de sanidade\n",
    "                    age = (reference_date - date).days / 365.25\n",
    "                    if 0 <= age <= 120:  # Idade v\u00e1lida\n",
    "                        return date\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return pd.NaT\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_age(birth_date: pd.Timestamp, reference_date: datetime = None) -> float:\n",
    "        \"\"\"Calcula idade com valida\u00e7\u00f5es.\"\"\"\n",
    "        if reference_date is None:\n",
    "            reference_date = datetime.now()\n",
    "        \n",
    "        if pd.isna(birth_date):\n",
    "            return np.nan\n",
    "        \n",
    "        age = (reference_date - birth_date).days / 365.25\n",
    "        \n",
    "        # Valida\u00e7\u00f5es\n",
    "        if age < 0 or age > 120:\n",
    "            return np.nan\n",
    "        \n",
    "        return age\n",
    "\n",
    "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Feature Engineering Enterprise com t\u00e9cnicas avan\u00e7adas.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MLConfig):\n",
    "        self.config = config\n",
    "        self.feature_stats_ = {}\n",
    "        self.created_features_ = []\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"Aprende estat\u00edsticas das features.\"\"\"\n",
    "        logger.info(\"Aprendendo estat\u00edsticas para feature engineering...\")\n",
    "        \n",
    "        # Estat\u00edsticas para features num\u00e9ricas\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            self.feature_stats_[col] = {\n",
    "                'mean': X[col].mean(),\n",
    "                'median': X[col].median(),\n",
    "                'std': X[col].std(),\n",
    "                'q1': X[col].quantile(0.25),\n",
    "                'q3': X[col].quantile(0.75)\n",
    "            }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aplica engenharia de features.\"\"\"\n",
    "        logger.info(\"Aplicando feature engineering avan\u00e7ado...\")\n",
    "        X_eng = X.copy()\n",
    "        \n",
    "        # 1. Tratamento de Datas\n",
    "        X_eng = self._engineer_date_features(X_eng)\n",
    "        \n",
    "        # 2. Features de Raz\u00e3o e Propor\u00e7\u00e3o\n",
    "        X_eng = self._engineer_ratio_features(X_eng)\n",
    "        \n",
    "        # 3. Features de Agrega\u00e7\u00e3o\n",
    "        X_eng = self._engineer_aggregation_features(X_eng)\n",
    "        \n",
    "        # 4. Features de Intera\u00e7\u00e3o\n",
    "        if self.config.create_interaction_features:\n",
    "            X_eng = self._engineer_interaction_features(X_eng)\n",
    "        \n",
    "        # 5. Features Polinomiais\n",
    "        if self.config.create_polynomial_features:\n",
    "            X_eng = self._engineer_polynomial_features(X_eng)\n",
    "        \n",
    "        # 6. Features de Comportamento\n",
    "        X_eng = self._engineer_behavioral_features(X_eng)\n",
    "        \n",
    "        # 7. Features de Risco\n",
    "        X_eng = self._engineer_risk_features(X_eng)\n",
    "        \n",
    "        # Remove infinitos e NaN resultantes\n",
    "        X_eng = X_eng.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        logger.info(f\"\u2705 Features criadas: {len(self.created_features_)}\")\n",
    "        logger.info(f\"   Shape final: {X_eng.shape}\")\n",
    "        \n",
    "        return X_eng\n",
    "    \n",
    "    def _engineer_date_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Processa features de data.\"\"\"\n",
    "        if 'nascimento' in X.columns:\n",
    "            logger.info(\"   \u2192 Processando datas de nascimento...\")\n",
    "            \n",
    "            # Parse robusto de datas\n",
    "            birth_dates = X['nascimento'].apply(\n",
    "                lambda x: SmartDateParser.parse_date(x)\n",
    "            )\n",
    "            \n",
    "            # Calcula idade\n",
    "            X['idade'] = birth_dates.apply(\n",
    "                lambda x: SmartDateParser.calculate_age(x)\n",
    "            )\n",
    "            \n",
    "            # Features derivadas de idade\n",
    "            X['idade_categoria'] = pd.cut(\n",
    "                X['idade'],\n",
    "                bins=[0, 25, 35, 50, 65, 100],\n",
    "                labels=['jovem', 'adulto_jovem', 'adulto', 'senior', 'aposentado']\n",
    "            )\n",
    "            \n",
    "            X['geracao'] = pd.cut(\n",
    "                X['idade'],\n",
    "                bins=[0, 28, 44, 60, 100],\n",
    "                labels=['gen_z', 'millennial', 'gen_x', 'boomer']\n",
    "            )\n",
    "            \n",
    "            # Remove coluna original\n",
    "            X = X.drop(columns=['nascimento'])\n",
    "            self.created_features_.extend(['idade', 'idade_categoria', 'geracao'])\n",
    "        \n",
    "        # Data de efetiva\u00e7\u00e3o\n",
    "        if 'data_efetivacao' in X.columns:\n",
    "            X['data_efetivacao'] = pd.to_datetime(X['data_efetivacao'], errors='coerce')\n",
    "            \n",
    "            X['ano_efetivacao'] = X['data_efetivacao'].dt.year\n",
    "            X['mes_efetivacao'] = X['data_efetivacao'].dt.month\n",
    "            X['trimestre_efetivacao'] = X['data_efetivacao'].dt.quarter\n",
    "            X['dia_semana_efetivacao'] = X['data_efetivacao'].dt.dayofweek\n",
    "            X['eh_final_de_semana'] = X['dia_semana_efetivacao'].isin([5, 6]).astype(int)\n",
    "            X['eh_inicio_mes'] = (X['data_efetivacao'].dt.day <= 5).astype(int)\n",
    "            X['eh_fim_mes'] = (X['data_efetivacao'].dt.day >= 25).astype(int)\n",
    "            \n",
    "            self.created_features_.extend([\n",
    "                'ano_efetivacao', 'mes_efetivacao', 'trimestre_efetivacao',\n",
    "                'dia_semana_efetivacao', 'eh_final_de_semana',\n",
    "                'eh_inicio_mes', 'eh_fim_mes'\n",
    "            ])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _engineer_ratio_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Cria features de raz\u00e3o.\"\"\"\n",
    "        epsilon = 1e-6\n",
    "        \n",
    "        # Raz\u00e3o valor/parcelas\n",
    "        if 'total_financiado' in X.columns and 'quantidade_parcelas' in X.columns:\n",
    "            X['valor_medio_parcela'] = (\n",
    "                X['total_financiado'] / (X['quantidade_parcelas'] + epsilon)\n",
    "            )\n",
    "            self.created_features_.append('valor_medio_parcela')\n",
    "        \n",
    "        # Comprometimento de renda (LTV-like)\n",
    "        if 'total_financiado' in X.columns and 'renda_declarada' in X.columns:\n",
    "            X['razao_emprestimo_renda'] = (\n",
    "                X['total_financiado'] / (X['renda_declarada'] + epsilon)\n",
    "            )\n",
    "            \n",
    "            X['comprometimento_mensal'] = (\n",
    "                X.get('valor_medio_parcela', 0) / (X['renda_declarada'] + epsilon)\n",
    "            )\n",
    "            \n",
    "            self.created_features_.extend([\n",
    "                'razao_emprestimo_renda',\n",
    "                'comprometimento_mensal'\n",
    "            ])\n",
    "        \n",
    "        # Entrada como propor\u00e7\u00e3o\n",
    "        if 'entrada' in X.columns and 'total_financiado' in X.columns:\n",
    "            X['proporcao_entrada'] = (\n",
    "                X['entrada'] / (X['total_financiado'] + epsilon)\n",
    "            )\n",
    "            self.created_features_.append('proporcao_entrada')\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _engineer_aggregation_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Features de agrega\u00e7\u00e3o por grupos.\"\"\"\n",
    "        \n",
    "        # Por cidade\n",
    "        if 'endereco_cidade' in X.columns:\n",
    "            city_stats = X.groupby('endereco_cidade').agg({\n",
    "                col: ['mean', 'std', 'count'] \n",
    "                for col in X.select_dtypes(include=[np.number]).columns[:3]\n",
    "            })\n",
    "            \n",
    "            # Simplified aggregation\n",
    "            if 'total_financiado' in X.columns:\n",
    "                city_avg = X.groupby('endereco_cidade')['total_financiado'].transform('mean')\n",
    "                X['valor_vs_media_cidade'] = X['total_financiado'] / (city_avg + 1e-6)\n",
    "                self.created_features_.append('valor_vs_media_cidade')\n",
    "        \n",
    "        # Por profiss\u00e3o\n",
    "        if 'profissao' in X.columns and 'renda_declarada' in X.columns:\n",
    "            prof_avg_income = X.groupby('profissao')['renda_declarada'].transform('mean')\n",
    "            X['renda_vs_media_profissao'] = X['renda_declarada'] / (prof_avg_income + 1e-6)\n",
    "            self.created_features_.append('renda_vs_media_profissao')\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _engineer_interaction_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Features de intera\u00e7\u00e3o entre vari\u00e1veis importantes.\"\"\"\n",
    "        \n",
    "        # Idade \u00d7 Valor do empr\u00e9stimo\n",
    "        if 'idade' in X.columns and 'total_financiado' in X.columns:\n",
    "            X['idade_x_valor'] = X['idade'] * X['total_financiado']\n",
    "            self.created_features_.append('idade_x_valor')\n",
    "        \n",
    "        # Prazo \u00d7 Valor parcela\n",
    "        if 'quantidade_parcelas' in X.columns and 'valor_medio_parcela' in X.columns:\n",
    "            X['prazo_x_parcela'] = X['quantidade_parcelas'] * X['valor_medio_parcela']\n",
    "            self.created_features_.append('prazo_x_parcela')\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _engineer_polynomial_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Features polinomiais para capturar n\u00e3o-linearidades.\"\"\"\n",
    "        \n",
    "        poly_candidates = ['idade', 'total_financiado', 'quantidade_parcelas']\n",
    "        poly_candidates = [c for c in poly_candidates if c in X.columns]\n",
    "        \n",
    "        for col in poly_candidates[:2]:  # Limita para evitar explos\u00e3o dimensional\n",
    "            if X[col].dtype in [np.float64, np.int64]:\n",
    "                X[f'{col}_squared'] = X[col] ** 2\n",
    "                X[f'{col}_sqrt'] = np.sqrt(np.abs(X[col]))\n",
    "                self.created_features_.extend([f'{col}_squared', f'{col}_sqrt'])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _engineer_behavioral_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Features comportamentais.\"\"\"\n",
    "        \n",
    "        # Flag cliente novo\n",
    "        if 'quantidade_parcelas' in X.columns:\n",
    "            X['eh_cliente_novo'] = (X['quantidade_parcelas'] <= 2).astype(int)\n",
    "            self.created_features_.append('eh_cliente_novo')\n",
    "        \n",
    "        # Perfil de risco baseado em m\u00faltiplas vari\u00e1veis\n",
    "        if 'idade' in X.columns and 'total_financiado' in X.columns:\n",
    "            # Cliente jovem + valor alto = maior risco\n",
    "            X['perfil_risco_idade_valor'] = (\n",
    "                (X['idade'] < 25) & (X['total_financiado'] > X['total_financiado'].quantile(0.75))\n",
    "            ).astype(int)\n",
    "            self.created_features_.append('perfil_risco_idade_valor')\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _engineer_risk_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Features espec\u00edficas para scoring de risco.\"\"\"\n",
    "        \n",
    "        # Prazo longo = maior risco\n",
    "        if 'quantidade_parcelas' in X.columns:\n",
    "            X['eh_prazo_longo'] = (X['quantidade_parcelas'] > 48).astype(int)\n",
    "            self.created_features_.append('eh_prazo_longo')\n",
    "        \n",
    "        # Comprometimento alto\n",
    "        if 'comprometimento_mensal' in X.columns:\n",
    "            X['comprometimento_alto'] = (X['comprometimento_mensal'] > 0.3).astype(int)\n",
    "            self.created_features_.append('comprometimento_alto')\n",
    "        \n",
    "        return X\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. PIPELINE DE PR\u00c9-PROCESSAMENTO ROBUSTO\n",
    "# ==============================================================================\n",
    "\n",
    "class RobustOutlierTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Tratamento robusto de outliers usando IQR.\"\"\"\n",
    "    \n",
    "    def __init__(self, factor=1.5, method='winsorize'):\n",
    "        self.factor = factor\n",
    "        self.method = method  # 'winsorize', 'clip', 'remove'\n",
    "        self.bounds_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        \n",
    "        for col in X_df.columns:\n",
    "            if X_df[col].dtype in [np.float64, np.int64]:\n",
    "                Q1 = X_df[col].quantile(0.25)\n",
    "                Q3 = X_df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                self.bounds_[col] = {\n",
    "                    'lower': Q1 - self.factor * IQR,\n",
    "                    'upper': Q3 + self.factor * IQR\n",
    "                }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X).copy()\n",
    "        \n",
    "        for col, bounds in self.bounds_.items():\n",
    "            if col in X_df.columns:\n",
    "                if self.method == 'winsorize':\n",
    "                    X_df[col] = X_df[col].clip(\n",
    "                        lower=bounds['lower'],\n",
    "                        upper=bounds['upper']\n",
    "                    )\n",
    "        \n",
    "        return X_df.values\n",
    "\n",
    "class SmartImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Imputador inteligente que escolhe estrat\u00e9gia por coluna.\"\"\"\n",
    "    \n",
    "    def __init__(self, numeric_strategy='knn', categorical_strategy='mode'):\n",
    "        self.numeric_strategy = numeric_strategy\n",
    "        self.categorical_strategy = categorical_strategy\n",
    "        self.imputers_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        \n",
    "        # Imputer num\u00e9rico\n",
    "        numeric_cols = X_df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            if self.numeric_strategy == 'knn':\n",
    "                self.imputers_['numeric'] = KNNImputer(n_neighbors=5)\n",
    "            else:\n",
    "                self.imputers_['numeric'] = SimpleImputer(strategy=self.numeric_strategy)\n",
    "            \n",
    "            self.imputers_['numeric'].fit(X_df[numeric_cols])\n",
    "        \n",
    "        # Imputer categ\u00f3rico\n",
    "        categorical_cols = X_df.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            self.imputers_['categorical'] = SimpleImputer(\n",
    "                strategy='most_frequent'\n",
    "            )\n",
    "            self.imputers_['categorical'].fit(X_df[categorical_cols])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X).copy()\n",
    "        \n",
    "        # Imputa num\u00e9ricos\n",
    "        numeric_cols = X_df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0 and 'numeric' in self.imputers_:\n",
    "            X_df[numeric_cols] = self.imputers_['numeric'].transform(X_df[numeric_cols])\n",
    "        \n",
    "        # Imputa categ\u00f3ricos\n",
    "        categorical_cols = X_df.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(categorical_cols) > 0 and 'categorical' in self.imputers_:\n",
    "            X_df[categorical_cols] = self.imputers_['categorical'].transform(X_df[categorical_cols])\n",
    "        \n",
    "        return X_df\n",
    "\n",
    "def build_preprocessing_pipeline(config: MLConfig) -> Pipeline:\n",
    "    \"\"\"Constr\u00f3i pipeline de pr\u00e9-processamento robusto.\"\"\"\n",
    "    \n",
    "    steps = [\n",
    "        ('feature_engineering', AdvancedFeatureEngineer(config)),\n",
    "        ('outlier_treatment', RobustOutlierTransformer(factor=1.5)),\n",
    "        ('imputation', SmartImputer(\n",
    "            numeric_strategy=config.numeric_imputation_strategy,\n",
    "            categorical_strategy=config.categorical_imputation_strategy\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    # Adiciona scaler se necess\u00e1rio\n",
    "    if config.scaling_method == 'robust':\n",
    "        steps.append(('scaling', RobustScaler()))\n",
    "    elif config.scaling_method == 'standard':\n",
    "        steps.append(('scaling', StandardScaler()))\n",
    "    \n",
    "    return Pipeline(steps)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. GEST\u00c3O DE DADOS TEMPORAIS\n",
    "# ==============================================================================\n",
    "\n",
    "class TemporalDataSplitter:\n",
    "    \"\"\"Split temporal para evitar data leakage.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MLConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def split(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Tuple]:\n",
    "        \"\"\"\n",
    "        Realiza split temporal garantindo que:\n",
    "        - Train: dados mais antigos\n",
    "        - Validation: per\u00edodo intermedi\u00e1rio\n",
    "        - Test: per\u00edodo recente\n",
    "        - OOT: \u00faltimos N meses (completamente fora da amostra)\n",
    "        \"\"\"\n",
    "        logger.info(\"Realizando split temporal dos dados...\")\n",
    "        \n",
    "        date_col = self.config.date_column\n",
    "        \n",
    "        if date_col not in X.columns:\n",
    "            logger.warning(f\"Coluna de data '{date_col}' n\u00e3o encontrada. Usando split aleat\u00f3rio estratificado.\")\n",
    "            return self._random_stratified_split(X, y)\n",
    "        \n",
    "        # Ordena por data\n",
    "        X_sorted = X.sort_values(date_col).reset_index(drop=True)\n",
    "        y_sorted = y.loc[X_sorted.index].reset_index(drop=True)\n",
    "        \n",
    "        # Define cutoffs temporais\n",
    "        max_date = X_sorted[date_col].max()\n",
    "        \n",
    "        # OOT: \u00faltimos N meses\n",
    "        oot_cutoff = max_date - pd.DateOffset(months=self.config.oot_months)\n",
    "        \n",
    "        oot_mask = X_sorted[date_col] >= oot_cutoff\n",
    "        train_val_test_mask = ~oot_mask\n",
    "        \n",
    "        # Separa OOT\n",
    "        X_oot = X_sorted[oot_mask].reset_index(drop=True)\n",
    "        y_oot = y_sorted[oot_mask].reset_index(drop=True)\n",
    "        \n",
    "        X_remaining = X_sorted[train_val_test_mask].reset_index(drop=True)\n",
    "        y_remaining = y_sorted[train_val_test_mask].reset_index(drop=True)\n",
    "        \n",
    "        # Split do restante\n",
    "        n_remaining = len(X_remaining)\n",
    "        n_test = int(n_remaining * self.config.test_size)\n",
    "        n_val = int(n_remaining * self.config.validation_size)\n",
    "        n_train = n_remaining - n_test - n_val\n",
    "        \n",
    "        X_train = X_remaining.iloc[:n_train]\n",
    "        y_train = y_remaining.iloc[:n_train]\n",
    "        \n",
    "        X_val = X_remaining.iloc[n_train:n_train+n_val]\n",
    "        y_val = y_remaining.iloc[n_train:n_train+n_val]\n",
    "        \n",
    "        X_test = X_remaining.iloc[n_train+n_val:]\n",
    "        y_test = y_remaining.iloc[n_train+n_val:]\n",
    "        \n",
    "        logger.info(f\"\u2705 Split temporal realizado:\")\n",
    "        logger.info(f\"   Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "        logger.info(f\"   Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "        logger.info(f\"   Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "        logger.info(f\"   OOT: {len(X_oot)} ({len(X_oot)/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train, y_train),\n",
    "            'validation': (X_val, y_val),\n",
    "            'test': (X_test, y_test),\n",
    "            'oot': (X_oot, y_oot)\n",
    "        }\n",
    "    \n",
    "    def _random_stratified_split(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Tuple]:\n",
    "        \"\"\"Fallback para split estratificado aleat\u00f3rio.\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Train + Val vs Test + OOT\n",
    "        X_train_val, X_test_oot, y_train_val, y_test_oot = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.config.test_size + self.config.validation_size + 0.10,\n",
    "            stratify=y,\n",
    "            random_state=self.config.random_state\n",
    "        )\n",
    "        \n",
    "        # Train vs Val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=self.config.validation_size / (1 - self.config.test_size - 0.10),\n",
    "            stratify=y_train_val,\n",
    "            random_state=self.config.random_state\n",
    "        )\n",
    "        \n",
    "        # Test vs OOT\n",
    "        X_test, X_oot, y_test, y_oot = train_test_split(\n",
    "            X_test_oot, y_test_oot,\n",
    "            test_size=0.33,\n",
    "            stratify=y_test_oot,\n",
    "            random_state=self.config.random_state\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train, y_train),\n",
    "            'validation': (X_val, y_val),\n",
    "            'test': (X_test, y_test),\n",
    "            'oot': (X_oot, y_oot)\n",
    "        }\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. M\u00c9TRICAS E AVALIA\u00c7\u00c3O ENTERPRISE\n",
    "# ==============================================================================\n",
    "\n",
    "class CreditScoringMetrics:\n",
    "    \"\"\"Calculadora completa de m\u00e9tricas para credit scoring.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_all_metrics(y_true: np.ndarray, \n",
    "                             y_prob: np.ndarray,\n",
    "                             y_pred: np.ndarray = None,\n",
    "                             threshold: float = 0.5) -> Dict[str, float]:\n",
    "        \"\"\"Calcula todas as m\u00e9tricas relevantes.\"\"\"\n",
    "        \n",
    "        if y_pred is None:\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        # M\u00e9tricas de discrimina\u00e7\u00e3o\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        gini = 2 * auc - 1\n",
    "        \n",
    "        # KS Statistic\n",
    "        ks_stat, ks_pvalue = ks_2samp(\n",
    "            y_prob[y_true == 0],\n",
    "            y_prob[y_true == 1]\n",
    "        )\n",
    "        \n",
    "        # M\u00e9tricas de calibra\u00e7\u00e3o\n",
    "        brier = brier_score_loss(y_true, y_prob)\n",
    "        logloss = log_loss(y_true, y_prob)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        # M\u00e9tricas de classifica\u00e7\u00e3o\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        # M\u00e9tricas de neg\u00f3cio\n",
    "        approval_rate = (y_pred == 0).mean()  # Assumindo 0 = aprovado\n",
    "        bad_rate = y_true[y_pred == 0].mean() if sum(y_pred == 0) > 0 else 0\n",
    "        \n",
    "        # Average Precision (\u00fatil para dados desbalanceados)\n",
    "        avg_precision = average_precision_score(y_true, y_prob)\n",
    "        \n",
    "        return {\n",
    "            # Discrimina\u00e7\u00e3o\n",
    "            'auc': auc,\n",
    "            'gini': gini,\n",
    "            'ks_statistic': ks_stat,\n",
    "            'ks_pvalue': ks_pvalue,\n",
    "            \n",
    "            # Calibra\u00e7\u00e3o\n",
    "            'brier_score': brier,\n",
    "            'log_loss': logloss,\n",
    "            \n",
    "            # Classifica\u00e7\u00e3o\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'avg_precision': avg_precision,\n",
    "            \n",
    "            # Confusion Matrix\n",
    "            'true_negatives': int(tn),\n",
    "            'false_positives': int(fp),\n",
    "            'false_negatives': int(fn),\n",
    "            'true_positives': int(tp),\n",
    "            \n",
    "            # Neg\u00f3cio\n",
    "            'approval_rate': approval_rate,\n",
    "            'bad_rate': bad_rate,\n",
    "            'threshold': threshold\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_business_value(metrics: Dict[str, float],\n",
    "                                business_config: BusinessMetrics) -> Dict[str, float]:\n",
    "        \"\"\"Calcula valor de neg\u00f3cio do modelo.\"\"\"\n",
    "        \n",
    "        tn = metrics['true_negatives']\n",
    "        fp = metrics['false_positives']\n",
    "        fn = metrics['false_negatives']\n",
    "        tp = metrics['true_positives']\n",
    "        \n",
    "        # Receita de clientes bons aprovados\n",
    "        revenue_good_clients = tn * business_config.revenue_per_good_client\n",
    "        \n",
    "        # Perda de clientes maus aprovados\n",
    "        loss_bad_clients = fp * business_config.loss_per_bad_client\n",
    "        \n",
    "        # Custo operacional\n",
    "        total_analyzed = tn + fp + fn + tp\n",
    "        operational_cost = total_analyzed * business_config.operational_cost_per_analysis\n",
    "        \n",
    "        # Profit total\n",
    "        total_profit = revenue_good_clients - loss_bad_clients - operational_cost\n",
    "        \n",
    "        # Profit por cliente analisado\n",
    "        profit_per_client = total_profit / total_analyzed if total_analyzed > 0 else 0\n",
    "        \n",
    "        # ROI\n",
    "        roi = (total_profit / operational_cost - 1) * 100 if operational_cost > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_revenue': revenue_good_clients,\n",
    "            'total_losses': loss_bad_clients,\n",
    "            'operational_cost': operational_cost,\n",
    "            'net_profit': total_profit,\n",
    "            'profit_per_client': profit_per_client,\n",
    "            'roi_percentage': roi,\n",
    "            'clients_approved': tn + fp,\n",
    "            'clients_rejected': fn + tp\n",
    "        }\n",
    "\n",
    "class PSICalculator:\n",
    "    \"\"\"Calculador de Population Stability Index (PSI).\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_psi(expected: np.ndarray, \n",
    "                     actual: np.ndarray,\n",
    "                     buckets: int = 10,\n",
    "                     epsilon: float = 0.0001) -> Tuple[float, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Calcula PSI entre duas distribui\u00e7\u00f5es.\n",
    "        \n",
    "        Interpreta\u00e7\u00e3o:\n",
    "        - PSI < 0.10: Sem mudan\u00e7a significativa\n",
    "        - 0.10 <= PSI < 0.25: Mudan\u00e7a moderada, investigar\n",
    "        - PSI >= 0.25: Mudan\u00e7a significativa, retreino necess\u00e1rio\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remove NaN\n",
    "        expected = expected[~np.isnan(expected)]\n",
    "        actual = actual[~np.isnan(actual)]\n",
    "        \n",
    "        # Define bins baseado na distribui\u00e7\u00e3o esperada\n",
    "        breakpoints = np.percentile(expected, np.linspace(0, 100, buckets + 1))\n",
    "        breakpoints = np.unique(breakpoints)  # Remove duplicatas\n",
    "        \n",
    "        # Conta elementos em cada bucket\n",
    "        expected_percents = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "        actual_percents = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "        \n",
    "        # Adiciona epsilon para evitar log(0)\n",
    "        expected_percents = np.clip(expected_percents, epsilon, 1)\n",
    "        actual_percents = np.clip(actual_percents, epsilon, 1)\n",
    "        \n",
    "        # Calcula PSI\n",
    "        psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n",
    "        psi_total = np.sum(psi_values)\n",
    "        \n",
    "        # DataFrame com detalhes\n",
    "        psi_df = pd.DataFrame({\n",
    "            'bucket': range(len(psi_values)),\n",
    "            'expected_percent': expected_percents,\n",
    "            'actual_percent': actual_percents,\n",
    "            'psi_contribution': psi_values\n",
    "        })\n",
    "        \n",
    "        return psi_total, psi_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_feature_psi(X_expected: pd.DataFrame,\n",
    "                             X_actual: pd.DataFrame,\n",
    "                             buckets: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Calcula PSI para todas as features.\"\"\"\n",
    "        \n",
    "        psi_results = []\n",
    "        \n",
    "        numeric_cols = X_expected.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            try:\n",
    "                psi, _ = PSICalculator.calculate_psi(\n",
    "                    X_expected[col].values,\n",
    "                    X_actual[col].values,\n",
    "                    buckets=buckets\n",
    "                )\n",
    "                \n",
    "                psi_results.append({\n",
    "                    'feature': col,\n",
    "                    'psi': psi,\n",
    "                    'status': 'OK' if psi < 0.10 else ('WARNING' if psi < 0.25 else 'CRITICAL')\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erro ao calcular PSI para {col}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(psi_results).sort_values('psi', ascending=False)\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. OTIMIZA\u00c7\u00c3O DE HIPERPAR\u00c2METROS INTELIGENTE\n",
    "# ==============================================================================\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    \"\"\"Otimizador Bayesiano com Optuna para m\u00faltiplos modelos.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EnterpriseConfig):\n",
    "        self.config = config\n",
    "        self.study = None\n",
    "        self.best_model_type = None\n",
    "    \n",
    "    def optimize(self,\n",
    "                X_train: pd.DataFrame,\n",
    "                y_train: pd.Series,\n",
    "                X_val: pd.DataFrame,\n",
    "                y_val: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"Otimiza hiperpar\u00e2metros para m\u00faltiplos modelos.\"\"\"\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"INICIANDO OTIMIZA\u00c7\u00c3O BAYESIANA DE HIPERPAR\u00c2METROS\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        # Cria estudo Optuna\n",
    "        self.study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=f'credit_scoring_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "            sampler=optuna.samplers.TPESampler(seed=self.config.ml_config.random_state)\n",
    "        )\n",
    "        \n",
    "        # Fun\u00e7\u00e3o objetivo que ser\u00e1 otimizada\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            # Escolhe tipo de modelo\n",
    "            model_type = trial.suggest_categorical(\n",
    "                'model_type',\n",
    "                self.config.ml_config.models_to_evaluate\n",
    "            )\n",
    "            \n",
    "            # Obt\u00e9m hiperpar\u00e2metros espec\u00edficos do modelo\n",
    "            if model_type == 'xgboost':\n",
    "                params = self._get_xgboost_params(trial, y_train)\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "            \n",
    "            elif model_type == 'lightgbm':\n",
    "                params = self._get_lightgbm_params(trial, y_train)\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "            \n",
    "            elif model_type == 'ensemble':\n",
    "                # Ensemble de XGBoost + LightGBM\n",
    "                xgb_params = self._get_xgboost_params(trial, y_train, prefix='xgb_')\n",
    "                lgb_params = self._get_lightgbm_params(trial, y_train, prefix='lgb_')\n",
    "                \n",
    "                model = VotingEnsemble(\n",
    "                    xgb.XGBClassifier(**xgb_params),\n",
    "                    lgb.LGBMClassifier(**lgb_params),\n",
    "                    weights=trial.suggest_float('ensemble_weight_xgb', 0.3, 0.7)\n",
    "                )\n",
    "            \n",
    "            # Treina modelo\n",
    "            try:\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)] if hasattr(model, 'eval_set') else None,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # Predi\u00e7\u00e3o\n",
    "                y_prob = model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "                # M\u00e9trica objetivo: AUC\n",
    "                auc = roc_auc_score(y_val, y_prob)\n",
    "                \n",
    "                # Log progresso\n",
    "                trial_num = trial.number + 1\n",
    "                total_trials = self.config.ml_config.n_trials_optuna\n",
    "                logger.info(f\"Trial {trial_num}/{total_trials} | Modelo: {model_type} | AUC: {auc:.4f}\")\n",
    "                \n",
    "                return auc\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro no trial {trial.number}: {e}\")\n",
    "                return 0.0\n",
    "        \n",
    "        # Executa otimiza\u00e7\u00e3o\n",
    "        self.study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.config.ml_config.n_trials_optuna,\n",
    "            timeout=self.config.ml_config.optuna_timeout,\n",
    "            show_progress_bar=True,\n",
    "            callbacks=[self._logging_callback]\n",
    "        )\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"\u2705 OTIMIZA\u00c7\u00c3O CONCLU\u00cdDA\")\n",
    "        logger.info(f\"   Melhor AUC: {self.study.best_value:.4f}\")\n",
    "        logger.info(f\"   Melhor modelo: {self.study.best_params.get('model_type', 'N/A')}\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'best_params': self.study.best_params,\n",
    "            'best_value': self.study.best_value,\n",
    "            'best_trial': self.study.best_trial.number,\n",
    "            'optimization_history': self._get_optimization_history()\n",
    "        }\n",
    "    \n",
    "    def _get_xgboost_params(self, trial: optuna.Trial, y_train: pd.Series, prefix: str = '') -> Dict:\n",
    "        \"\"\"Define espa\u00e7o de busca para XGBoost.\"\"\"\n",
    "        \n",
    "        # Calcula scale_pos_weight para desbalanceamento\n",
    "        class_counts = y_train.value_counts()\n",
    "        scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "        \n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int(f'{prefix}n_estimators', 100, 1000, step=100),\n",
    "            'max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float(f'{prefix}learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float(f'{prefix}subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float(f'{prefix}colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int(f'{prefix}min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_float(f'{prefix}gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_float(f'{prefix}reg_alpha', 0, 5),\n",
    "            'reg_lambda': trial.suggest_float(f'{prefix}reg_lambda', 0, 5),\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'random_state': self.config.ml_config.random_state,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist',\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "    \n",
    "    def _get_lightgbm_params(self, trial: optuna.Trial, y_train: pd.Series, prefix: str = '') -> Dict:\n",
    "        \"\"\"Define espa\u00e7o de busca para LightGBM.\"\"\"\n",
    "        \n",
    "        class_counts = y_train.value_counts()\n",
    "        scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "        \n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int(f'{prefix}n_estimators', 100, 1000, step=100),\n",
    "            'max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float(f'{prefix}learning_rate', 0.01, 0.3, log=True),\n",
    "            'num_leaves': trial.suggest_int(f'{prefix}num_leaves', 20, 150),\n",
    "            'subsample': trial.suggest_float(f'{prefix}subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float(f'{prefix}colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_samples': trial.suggest_int(f'{prefix}min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float(f'{prefix}reg_alpha', 0, 5),\n",
    "            'reg_lambda': trial.suggest_float(f'{prefix}reg_lambda', 0, 5),\n",
    "            'scale_pos_weight': scale_pos_weight,\n",
    "            'random_state': self.config.ml_config.random_state,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    def _logging_callback(self, study: optuna.Study, trial: optuna.Trial):\n",
    "        \"\"\"Callback para logging durante otimiza\u00e7\u00e3o.\"\"\"\n",
    "        if trial.number % 10 == 0:\n",
    "            logger.info(f\"Progress: {trial.number}/{self.config.ml_config.n_trials_optuna} trials completed\")\n",
    "    \n",
    "    def _get_optimization_history(self) -> pd.DataFrame:\n",
    "        \"\"\"Retorna hist\u00f3rico de otimiza\u00e7\u00e3o.\"\"\"\n",
    "        return self.study.trials_dataframe()\n",
    "\n",
    "class VotingEnsemble:\n",
    "    \"\"\"Ensemble simples de modelos com voting.\"\"\"\n",
    "    \n",
    "    def __init__(self, model1, model2, weights: float = 0.5):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.weight1 = weights\n",
    "        self.weight2 = 1 - weights\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model1.fit(X, y)\n",
    "        self.model2.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        prob1 = self.model1.predict_proba(X)\n",
    "        prob2 = self.model2.predict_proba(X)\n",
    "        return self.weight1 * prob1 + self.weight2 * prob2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. OTIMIZA\u00c7\u00c3O DE CUTOFF BASEADA EM NEG\u00d3CIO\n",
    "# ==============================================================================\n",
    "\n",
    "class CutoffOptimizer:\n",
    "    \"\"\"Otimiza threshold de decis\u00e3o baseado em m\u00e9tricas de neg\u00f3cio.\"\"\"\n",
    "    \n",
    "    def __init__(self, business_config: BusinessMetrics):\n",
    "        self.business_config = business_config\n",
    "        self.optimal_cutoff = None\n",
    "        self.cutoff_analysis = None\n",
    "    \n",
    "    def find_optimal_cutoff(self,\n",
    "                           y_true: np.ndarray,\n",
    "                           y_prob: np.ndarray,\n",
    "                           strategy: str = 'profit') -> float:\n",
    "        \"\"\"\n",
    "        Encontra cutoff \u00f3timo baseado em estrat\u00e9gia de neg\u00f3cio.\n",
    "        \n",
    "        Estrat\u00e9gias:\n",
    "        - 'profit': Maximiza lucro\n",
    "        - 'f1': Maximiza F1-score\n",
    "        - 'target_bad_rate': Atinge bad rate alvo\n",
    "        - 'max_approval': Maximiza aprova\u00e7\u00f5es mantendo bad rate aceit\u00e1vel\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Otimizando cutoff usando estrat\u00e9gia: {strategy}\")\n",
    "        \n",
    "        thresholds = np.linspace(0.05, 0.95, 200)\n",
    "        results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            \n",
    "            # Calcula m\u00e9tricas\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            \n",
    "            # M\u00e9tricas de classifica\u00e7\u00e3o\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # M\u00e9tricas de neg\u00f3cio\n",
    "            approval_rate = (y_pred == 0).mean()\n",
    "            bad_rate = fp / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            # C\u00e1lculo de profit\n",
    "            revenue = tn * self.business_config.revenue_per_good_client\n",
    "            losses = fp * self.business_config.loss_per_bad_client\n",
    "            costs = (tn + fp + fn + tp) * self.business_config.operational_cost_per_analysis\n",
    "            profit = revenue - losses - costs\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'profit': profit,\n",
    "                'f1_score': f1,\n",
    "                'approval_rate': approval_rate,\n",
    "                'bad_rate': bad_rate,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'true_negatives': tn,\n",
    "                'false_positives': fp,\n",
    "                'false_negatives': fn,\n",
    "                'true_positives': tp\n",
    "            })\n",
    "        \n",
    "        self.cutoff_analysis = pd.DataFrame(results)\n",
    "        \n",
    "        # Seleciona cutoff baseado na estrat\u00e9gia\n",
    "        if strategy == 'profit':\n",
    "            # Respeita max_approval_rate\n",
    "            valid = self.cutoff_analysis[\n",
    "                self.cutoff_analysis['approval_rate'] <= self.business_config.max_approval_rate\n",
    "            ]\n",
    "            if len(valid) > 0:\n",
    "                optimal_row = valid.loc[valid['profit'].idxmax()]\n",
    "            else:\n",
    "                optimal_row = self.cutoff_analysis.loc[self.cutoff_analysis['profit'].idxmax()]\n",
    "        \n",
    "        elif strategy == 'f1':\n",
    "            optimal_row = self.cutoff_analysis.loc[self.cutoff_analysis['f1_score'].idxmax()]\n",
    "        \n",
    "        elif strategy == 'target_bad_rate':\n",
    "            # Encontra cutoff que chega mais pr\u00f3ximo do bad_rate alvo\n",
    "            self.cutoff_analysis['distance_to_target'] = np.abs(\n",
    "                self.cutoff_analysis['bad_rate'] - self.business_config.target_bad_rate\n",
    "            )\n",
    "            optimal_row = self.cutoff_analysis.loc[\n",
    "                self.cutoff_analysis['distance_to_target'].idxmin()\n",
    "            ]\n",
    "        \n",
    "        elif strategy == 'max_approval':\n",
    "            # M\u00e1xima aprova\u00e7\u00e3o mantendo bad_rate <= target\n",
    "            valid = self.cutoff_analysis[\n",
    "                self.cutoff_analysis['bad_rate'] <= self.business_config.target_bad_rate\n",
    "            ]\n",
    "            if len(valid) > 0:\n",
    "                optimal_row = valid.loc[valid['approval_rate'].idxmax()]\n",
    "            else:\n",
    "                optimal_row = self.cutoff_analysis.loc[self.cutoff_analysis['approval_rate'].idxmax()]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Estrat\u00e9gia desconhecida: {strategy}\")\n",
    "        \n",
    "        self.optimal_cutoff = optimal_row['threshold']\n",
    "        \n",
    "        logger.info(f\"\u2705 Cutoff \u00f3timo encontrado: {self.optimal_cutoff:.4f}\")\n",
    "        logger.info(f\"   Approval Rate: {optimal_row['approval_rate']:.2%}\")\n",
    "        logger.info(f\"   Bad Rate: {optimal_row['bad_rate']:.2%}\")\n",
    "        logger.info(f\"   Profit: R$ {optimal_row['profit']:,.2f}\")\n",
    "        \n",
    "        return self.optimal_cutoff\n",
    "    \n",
    "    def plot_cutoff_analysis(self, save_path: Optional[Path] = None):\n",
    "        \"\"\"Plota an\u00e1lise de cutoff.\"\"\"\n",
    "        \n",
    "        if self.cutoff_analysis is None:\n",
    "            logger.warning(\"Execute find_optimal_cutoff primeiro\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Profit vs Threshold\n",
    "        axes[0, 0].plot(self.cutoff_analysis['threshold'], self.cutoff_analysis['profit'])\n",
    "        axes[0, 0].axvline(self.optimal_cutoff, color='r', linestyle='--', label='Optimal Cutoff')\n",
    "        axes[0, 0].set_xlabel('Threshold')\n",
    "        axes[0, 0].set_ylabel('Profit (R$)')\n",
    "        axes[0, 0].set_title('Profit vs Threshold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Approval Rate vs Bad Rate\n",
    "        axes[0, 1].plot(self.cutoff_analysis['approval_rate'], self.cutoff_analysis['bad_rate'])\n",
    "        axes[0, 1].axhline(self.business_config.target_bad_rate, color='g', linestyle='--', \n",
    "                          label='Target Bad Rate')\n",
    "        axes[0, 1].axvline(self.business_config.max_approval_rate, color='orange', linestyle='--',\n",
    "                          label='Max Approval Rate')\n",
    "        axes[0, 1].set_xlabel('Approval Rate')\n",
    "        axes[0, 1].set_ylabel('Bad Rate')\n",
    "        axes[0, 1].set_title('Approval Rate vs Bad Rate Trade-off')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # F1 Score vs Threshold\n",
    "        axes[1, 0].plot(self.cutoff_analysis['threshold'], self.cutoff_analysis['f1_score'])\n",
    "        axes[1, 0].axvline(self.optimal_cutoff, color='r', linestyle='--', label='Optimal Cutoff')\n",
    "        axes[1, 0].set_xlabel('Threshold')\n",
    "        axes[1, 0].set_ylabel('F1 Score')\n",
    "        axes[1, 0].set_title('F1 Score vs Threshold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Precision-Recall Trade-off\n",
    "        axes[1, 1].plot(self.cutoff_analysis['recall'], self.cutoff_analysis['precision'])\n",
    "        axes[1, 1].set_xlabel('Recall')\n",
    "        axes[1, 1].set_ylabel('Precision')\n",
    "        axes[1, 1].set_title('Precision-Recall Curve')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"An\u00e1lise de cutoff salva em: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. SISTEMA DE MONITORAMENTO E ALERTAS\n",
    "# ==============================================================================\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"Sistema de monitoramento cont\u00ednuo de performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EnterpriseConfig, baseline_metrics: Dict):\n",
    "        self.config = config\n",
    "        self.baseline_metrics = baseline_metrics\n",
    "        self.performance_history = []\n",
    "        self.alerts = []\n",
    "    \n",
    "    def evaluate_batch(self,\n",
    "                      model,\n",
    "                      X: pd.DataFrame,\n",
    "                      y_true: pd.Series,\n",
    "                      batch_date: datetime,\n",
    "                      X_baseline: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Avalia batch de produ\u00e7\u00e3o e detecta anomalias.\"\"\"\n",
    "        \n",
    "        logger.info(f\"Monitorando batch de {batch_date.strftime('%Y-%m-%d')}...\")\n",
    "        \n",
    "        # Predi\u00e7\u00f5es\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        y_pred = (y_prob >= self.config.thresholds.min_auc_acceptable).astype(int)\n",
    "        \n",
    "        # Calcula m\u00e9tricas atuais\n",
    "        current_metrics = CreditScoringMetrics.calculate_all_metrics(\n",
    "            y_true.values, y_prob, y_pred\n",
    "        )\n",
    "        current_metrics['batch_date'] = batch_date\n",
    "        current_metrics['sample_size'] = len(X)\n",
    "        \n",
    "        # Detecta degrada\u00e7\u00e3o de performance\n",
    "        self._check_performance_degradation(current_metrics)\n",
    "        \n",
    "        # Detecta data drift (PSI)\n",
    "        self._check_data_drift(X, X_baseline)\n",
    "        \n",
    "        # Detecta concept drift\n",
    "        self._check_concept_drift(current_metrics)\n",
    "        \n",
    "        # Registra hist\u00f3rico\n",
    "        self.performance_history.append(current_metrics)\n",
    "        \n",
    "        # Gera relat\u00f3rio\n",
    "        report = self._generate_monitoring_report(current_metrics)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _check_performance_degradation(self, current_metrics: Dict):\n",
    "        \"\"\"Detecta degrada\u00e7\u00e3o de performance.\"\"\"\n",
    "        \n",
    "        auc_drop = self.baseline_metrics['auc'] - current_metrics['auc']\n",
    "        \n",
    "        if auc_drop > 0.10:\n",
    "            self._create_alert(\n",
    "                AlertLevel.CRITICAL,\n",
    "                f\"AUC caiu {auc_drop:.3f} pontos! \"\n",
    "                f\"Baseline: {self.baseline_metrics['auc']:.4f}, \"\n",
    "                f\"Atual: {current_metrics['auc']:.4f}\"\n",
    "            )\n",
    "        elif auc_drop > 0.05:\n",
    "            self._create_alert(\n",
    "                AlertLevel.WARNING,\n",
    "                f\"AUC caiu {auc_drop:.3f} pontos. Monitorar de perto.\"\n",
    "            )\n",
    "        \n",
    "        # Verifica outras m\u00e9tricas cr\u00edticas\n",
    "        if current_metrics['auc'] < self.config.thresholds.min_auc_acceptable:\n",
    "            self._create_alert(\n",
    "                AlertLevel.EMERGENCY,\n",
    "                f\"AUC abaixo do m\u00ednimo aceit\u00e1vel! \"\n",
    "                f\"Atual: {current_metrics['auc']:.4f}, \"\n",
    "                f\"M\u00ednimo: {self.config.thresholds.min_auc_acceptable:.4f}\"\n",
    "            )\n",
    "        \n",
    "        if current_metrics['ks_statistic'] < self.config.thresholds.min_ks_acceptable:\n",
    "            self._create_alert(\n",
    "                AlertLevel.CRITICAL,\n",
    "                f\"KS Statistic abaixo do aceit\u00e1vel: {current_metrics['ks_statistic']:.4f}\"\n",
    "            )\n",
    "    \n",
    "    def _check_data_drift(self, X_current: pd.DataFrame, X_baseline: pd.DataFrame):\n",
    "        \"\"\"Detecta data drift usando PSI.\"\"\"\n",
    "        \n",
    "        feature_psi = PSICalculator.calculate_feature_psi(X_baseline, X_current)\n",
    "        \n",
    "        # Features com PSI cr\u00edtico\n",
    "        critical_features = feature_psi[feature_psi['status'] == 'CRITICAL']\n",
    "        \n",
    "        if len(critical_features) > 0:\n",
    "            self._create_alert(\n",
    "                AlertLevel.CRITICAL,\n",
    "                f\"Data drift cr\u00edtico detectado em {len(critical_features)} features: \"\n",
    "                f\"{critical_features['feature'].tolist()}\"\n",
    "            )\n",
    "        \n",
    "        # Features com PSI warning\n",
    "        warning_features = feature_psi[feature_psi['status'] == 'WARNING']\n",
    "        \n",
    "        if len(warning_features) > 0:\n",
    "            self._create_alert(\n",
    "                AlertLevel.WARNING,\n",
    "                f\"Data drift moderado em {len(warning_features)} features: \"\n",
    "                f\"{warning_features['feature'].tolist()}\"\n",
    "            )\n",
    "    \n",
    "    def _check_concept_drift(self, current_metrics: Dict):\n",
    "        \"\"\"Detecta concept drift (mudan\u00e7a na rela\u00e7\u00e3o X->y).\"\"\"\n",
    "        \n",
    "        if len(self.performance_history) < 5:\n",
    "            return  # Precisa de hist\u00f3rico\n",
    "        \n",
    "        # Analisa tend\u00eancia de AUC nos \u00faltimos 5 batches\n",
    "        recent_aucs = [m['auc'] for m in self.performance_history[-5:]]\n",
    "        \n",
    "        # Regress\u00e3o linear simples para detectar tend\u00eancia\n",
    "        x = np.arange(len(recent_aucs))\n",
    "        slope = np.polyfit(x, recent_aucs, 1)[0]\n",
    "        \n",
    "        if slope < -0.01:  # Queda de >1% por batch\n",
    "            self._create_alert(\n",
    "                AlertLevel.WARNING,\n",
    "                f\"Tend\u00eancia de queda na performance detectada. \"\n",
    "                f\"Slope: {slope:.4f}\"\n",
    "            )\n",
    "    \n",
    "    def _create_alert(self, level: AlertLevel, message: str):\n",
    "        \"\"\"Cria alerta.\"\"\"\n",
    "        \n",
    "        alert = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'level': level.value,\n",
    "            'message': message\n",
    "        }\n",
    "        \n",
    "        self.alerts.append(alert)\n",
    "        \n",
    "        # Log baseado no n\u00edvel\n",
    "        if level == AlertLevel.EMERGENCY or level == AlertLevel.CRITICAL:\n",
    "            logger.critical(f\"\ud83d\udea8 {message}\")\n",
    "        elif level == AlertLevel.WARNING:\n",
    "            logger.warning(f\"\u26a0\ufe0f  {message}\")\n",
    "        else:\n",
    "            logger.info(f\"\u2139\ufe0f  {message}\")\n",
    "    \n",
    "    def _generate_monitoring_report(self, current_metrics: Dict) -> Dict:\n",
    "        \"\"\"Gera relat\u00f3rio de monitoramento.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'current_metrics': current_metrics,\n",
    "            'baseline_metrics': self.baseline_metrics,\n",
    "            'alerts': self.alerts[-10:],  # \u00daltimos 10 alertas\n",
    "            'performance_trend': self._calculate_trend(),\n",
    "            'recommendation': self._get_recommendation()\n",
    "        }\n",
    "    \n",
    "    def _calculate_trend(self) -> str:\n",
    "        \"\"\"Calcula tend\u00eancia de performance.\"\"\"\n",
    "        \n",
    "        if len(self.performance_history) < 3:\n",
    "            return \"INSUFFICIENT_DATA\"\n",
    "        \n",
    "        recent_aucs = [m['auc'] for m in self.performance_history[-5:]]\n",
    "        \n",
    "        if len(recent_aucs) < 2:\n",
    "            return \"STABLE\"\n",
    "        \n",
    "        slope = np.polyfit(range(len(recent_aucs)), recent_aucs, 1)[0]\n",
    "        \n",
    "        if slope < -0.005:\n",
    "            return \"DECLINING\"\n",
    "        elif slope > 0.005:\n",
    "            return \"IMPROVING\"\n",
    "        else:\n",
    "            return \"STABLE\"\n",
    "    \n",
    "    def _get_recommendation(self) -> str:\n",
    "        \"\"\"Retorna recomenda\u00e7\u00e3o baseada em alertas.\"\"\"\n",
    "        \n",
    "        recent_alerts = self.alerts[-10:]\n",
    "        \n",
    "        critical_count = sum(1 for a in recent_alerts if a['level'] in ['critical', 'emergency'])\n",
    "        \n",
    "        if critical_count >= 3:\n",
    "            return \"IMMEDIATE_RETRAINING_REQUIRED\"\n",
    "        elif critical_count >= 1:\n",
    "            return \"SCHEDULE_RETRAINING\"\n",
    "        elif len([a for a in recent_alerts if a['level'] == 'warning']) >= 5:\n",
    "            return \"MONITOR_CLOSELY\"\n",
    "        else:\n",
    "            return \"CONTINUE_MONITORING\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. EXPLICABILIDADE COM SHAP\n",
    "# ==============================================================================\n",
    "\n",
    "class ModelExplainer:\n",
    "    \"\"\"Gerador de explica\u00e7\u00f5es do modelo usando SHAP.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, X_background: pd.DataFrame, max_samples: int = 100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Modelo treinado\n",
    "            X_background: Amostra de dados para background (treino)\n",
    "            max_samples: M\u00e1ximo de amostras para usar como background\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_background = X_background.sample(\n",
    "            n=min(max_samples, len(X_background)),\n",
    "            random_state=42\n",
    "        )\n",
    "        self.explainer = None\n",
    "        self.shap_values = None\n",
    "    \n",
    "    def compute_shap_values(self, X: pd.DataFrame):\n",
    "        \"\"\"Calcula SHAP values.\"\"\"\n",
    "        \n",
    "        logger.info(\"Calculando SHAP values para explicabilidade...\")\n",
    "        \n",
    "        # Cria explainer\n",
    "        self.explainer = shap.TreeExplainer(\n",
    "            self.model.named_steps['classifier'],\n",
    "            self.X_background\n",
    "        )\n",
    "        \n",
    "        # Calcula SHAP values\n",
    "        self.shap_values = self.explainer.shap_values(X)\n",
    "        \n",
    "        logger.info(\"\u2705 SHAP values calculados\")\n",
    "        \n",
    "        return self.shap_values\n",
    "    \n",
    "    def plot_summary(self, X: pd.DataFrame, save_path: Optional[Path] = None):\n",
    "        \"\"\"Plota summary plot do SHAP.\"\"\"\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            self.compute_shap_values(X)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(\n",
    "            self.shap_values,\n",
    "            X,\n",
    "            plot_type=\"bar\",\n",
    "            show=False\n",
    "        )\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"SHAP summary plot salvo em: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def get_feature_importance(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Retorna import\u00e2ncia de features via SHAP.\"\"\"\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            self.compute_shap_values(X)\n",
    "        \n",
    "        importance = np.abs(self.shap_values).mean(axis=0)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def explain_prediction(self, instance: pd.DataFrame, instance_idx: int = 0):\n",
    "        \"\"\"Explica uma predi\u00e7\u00e3o espec\u00edfica.\"\"\"\n",
    "        \n",
    "        if self.shap_values is None:\n",
    "            self.compute_shap_values(instance)\n",
    "        \n",
    "        shap.force_plot(\n",
    "            self.explainer.expected_value,\n",
    "            self.shap_values[instance_idx],\n",
    "            instance.iloc[instance_idx],\n",
    "            matplotlib=True\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 12. ARTEFATOS E PERSIST\u00caNCIA ENTERPRISE\n",
    "# ==============================================================================\n",
    "\n",
    "class ModelArtifact:\n",
    "    \"\"\"Gerenciador de artefatos do modelo (enterprise-grade).\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model: Pipeline,\n",
    "                 metadata: Dict,\n",
    "                 config: EnterpriseConfig,\n",
    "                 explainer: Optional[ModelExplainer] = None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.metadata = metadata\n",
    "        self.config = config\n",
    "        self.explainer = explainer\n",
    "        self.artifact_id = self._generate_artifact_id()\n",
    "        self.model_hash = self._compute_hash()\n",
    "    \n",
    "    def _generate_artifact_id(self) -> str:\n",
    "        \"\"\"Gera ID \u00fanico para o artefato.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return f\"credit_model_v{self.config.ml_config.model_version}_{timestamp}\"\n",
    "    \n",
    "    def _compute_hash(self) -> str:\n",
    "        \"\"\"Calcula hash SHA-256 do modelo.\"\"\"\n",
    "        model_bytes = joblib.dumps(self.model)\n",
    "        return hashlib.sha256(model_bytes).hexdigest()\n",
    "    \n",
    "    def save(self, base_path: Optional[Path] = None):\n",
    "        \"\"\"Salva artefato completo.\"\"\"\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = self.config.ml_config.output_dir\n",
    "        \n",
    "        artifact_dir = base_path / self.artifact_id\n",
    "        artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Salvando artefato em: {artifact_dir}\")\n",
    "        \n",
    "        # 1. Salva modelo\n",
    "        model_path = artifact_dir / 'model.joblib'\n",
    "        joblib.dump(self.model, model_path, compress=3)\n",
    "        logger.info(f\"   \u2705 Modelo salvo: {model_path}\")\n",
    "        \n",
    "        # 2. Salva metadados enriquecidos\n",
    "        enriched_metadata = self._enrich_metadata()\n",
    "        metadata_path = artifact_dir / 'metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(enriched_metadata, f, indent=4, default=str)\n",
    "        logger.info(f\"   \u2705 Metadados salvos: {metadata_path}\")\n",
    "        \n",
    "        # 3. Salva configura\u00e7\u00e3o\n",
    "        config_path = artifact_dir / 'config.json'\n",
    "        self.config.save(config_path)\n",
    "        logger.info(f\"   \u2705 Configura\u00e7\u00e3o salva: {config_path}\")\n",
    "        \n",
    "        # 4. Salva explainer (se dispon\u00edvel)\n",
    "        if self.explainer is not None:\n",
    "            explainer_path = artifact_dir / 'explainer.joblib'\n",
    "            joblib.dump(self.explainer, explainer_path)\n",
    "            logger.info(f\"   \u2705 Explainer salvo: {explainer_path}\")\n",
    "        \n",
    "        # 5. Gera Model Card (documenta\u00e7\u00e3o)\n",
    "        if self.config.ml_config.generate_model_card:\n",
    "            self._generate_model_card(artifact_dir)\n",
    "        \n",
    "        # 6. Cria arquivo de vers\u00e3o\n",
    "        version_info = {\n",
    "            'artifact_id': self.artifact_id,\n",
    "            'model_hash': self.model_hash,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'python_version': sys.version,\n",
    "            'dependencies': self._get_dependencies()\n",
    "        }\n",
    "        \n",
    "        version_path = artifact_dir / 'version.json'\n",
    "        with open(version_path, 'w') as f:\n",
    "            json.dump(version_info, f, indent=4)\n",
    "        logger.info(f\"   \u2705 Vers\u00e3o salva: {version_path}\")\n",
    "        \n",
    "        logger.info(f\"\u2705 Artefato completo salvo em: {artifact_dir}\")\n",
    "        \n",
    "        return artifact_dir\n",
    "    \n",
    "    def _enrich_metadata(self) -> Dict:\n",
    "        \"\"\"Enriquece metadados com informa\u00e7\u00f5es adicionais.\"\"\"\n",
    "        \n",
    "        enriched = self.metadata.copy()\n",
    "        enriched.update({\n",
    "            'artifact_id': self.artifact_id,\n",
    "            'model_hash': self.model_hash,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'model_version': self.config.ml_config.model_version,\n",
    "            'framework_versions': {\n",
    "                'xgboost': xgb.__version__,\n",
    "                'lightgbm': lgb.__version__,\n",
    "                'sklearn': __import__('sklearn').__version__,\n",
    "                'pandas': pd.__version__,\n",
    "                'numpy': np.__version__\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return enriched\n",
    "    \n",
    "    def _get_dependencies(self) -> Dict[str, str]:\n",
    "        \"\"\"Lista depend\u00eancias principais.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'xgboost': xgb.__version__,\n",
    "            'lightgbm': lgb.__version__,\n",
    "            'scikit-learn': __import__('sklearn').__version__,\n",
    "            'pandas': pd.__version__,\n",
    "            'numpy': np.__version__,\n",
    "            'shap': shap.__version__,\n",
    "            'optuna': optuna.__version__\n",
    "        }\n",
    "    \n",
    "    def _generate_model_card(self, artifact_dir: Path):\n",
    "        \"\"\"Gera Model Card (documenta\u00e7\u00e3o do modelo).\"\"\"\n",
    "        \n",
    "        model_card = f\"\"\"\n",
    "# Model Card: {self.artifact_id}\n",
    "\n",
    "## Informa\u00e7\u00f5es do Modelo\n",
    "\n",
    "- **Nome**: Credit Scoring Model\n",
    "- **Vers\u00e3o**: {self.config.ml_config.model_version}\n",
    "- **Tipo**: {self.metadata.get('model_type', 'Ensemble')}\n",
    "- **Data de Cria\u00e7\u00e3o**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Hash do Modelo**: {self.model_hash}\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Modelo de credit scoring para predi\u00e7\u00e3o de inadimpl\u00eancia em admiss\u00e3o de cr\u00e9dito.\n",
    "\n",
    "## Performance\n",
    "\n",
    "### M\u00e9tricas de Treino\n",
    "{self._format_metrics(self.metadata.get('train_metrics', {}))}\n",
    "\n",
    "### M\u00e9tricas de Valida\u00e7\u00e3o\n",
    "{self._format_metrics(self.metadata.get('validation_metrics', {}))}\n",
    "\n",
    "### M\u00e9tricas de Teste\n",
    "{self._format_metrics(self.metadata.get('test_metrics', {}))}\n",
    "\n",
    "### M\u00e9tricas Out-of-Time (OOT)\n",
    "{self._format_metrics(self.metadata.get('oot_metrics', {}))}\n",
    "\n",
    "## Cutoff \u00d3timo\n",
    "\n",
    "- **Threshold**: {self.metadata.get('optimal_cutoff', 0.5):.4f}\n",
    "- **Estrat\u00e9gia**: {self.metadata.get('cutoff_strategy', 'N/A')}\n",
    "\n",
    "## Features\n",
    "\n",
    "### Quantidade de Features\n",
    "- **Total**: {self.metadata.get('n_features', 'N/A')}\n",
    "- **Num\u00e9ricas**: {self.metadata.get('n_numeric_features', 'N/A')}\n",
    "- **Categ\u00f3ricas**: {self.metadata.get('n_categorical_features', 'N/A')}\n",
    "\n",
    "### Top 10 Features Mais Importantes\n",
    "{self._format_feature_importance()}\n",
    "\n",
    "## Dados de Treino\n",
    "\n",
    "- **Total de Registros**: {self.metadata.get('n_samples_train', 'N/A')}\n",
    "- **Per\u00edodo**: {self.metadata.get('train_period', 'N/A')}\n",
    "- **Balanceamento**: {self.metadata.get('class_balance', 'N/A')}\n",
    "\n",
    "## Valida\u00e7\u00f5es\n",
    "\n",
    "- **Data Leakage**: {self.metadata.get('leakage_check', 'PASSED')}\n",
    "- **PSI (Train vs OOT)**: {self.metadata.get('psi_train_oot', 'N/A')}\n",
    "- **Status de Drift**: {self.metadata.get('drift_status', 'OK')}\n",
    "\n",
    "## Compliance\n",
    "\n",
    "- **LGPD**: Compliant\n",
    "- **BACEN Res. 4.557/2017**: Compliant\n",
    "- **Audit Trail**: Enabled\n",
    "\n",
    "## Uso\n",
    "```python\n",
    "import joblib\n",
    "model = joblib.load('model.joblib')\n",
    "predictions = model.predict_proba(X_new)[:, 1]\n",
    "```\n",
    "\n",
    "## Contato\n",
    "\n",
    "Para d\u00favidas sobre este modelo, contate a equipe de Data Science.\n",
    "\n",
    "---\n",
    "*Gerado automaticamente em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "        \n",
    "        card_path = artifact_dir / 'MODEL_CARD.md'\n",
    "        with open(card_path, 'w') as f:\n",
    "            f.write(model_card)\n",
    "        \n",
    "        logger.info(f\"   \u2705 Model Card gerado: {card_path}\")\n",
    "    \n",
    "    def _format_metrics(self, metrics: Dict) -> str:\n",
    "        \"\"\"Formata m\u00e9tricas para Model Card.\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        lines = []\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                if key in ['auc', 'gini', 'ks_statistic', 'precision', 'recall', 'f1_score']:\n",
    "                    lines.append(f\"- **{key.upper()}**: {value:.4f}\")\n",
    "                elif key in ['approval_rate', 'bad_rate']:\n",
    "                    lines.append(f\"- **{key}**: {value:.2%}\")\n",
    "                else:\n",
    "                    lines.append(f\"- **{key}**: {value}\")\n",
    "        \n",
    "        return '\\n'.join(lines) if lines else \"N/A\"\n",
    "    \n",
    "    def _format_feature_importance(self) -> str:\n",
    "        \"\"\"Formata import\u00e2ncia de features para Model Card.\"\"\"\n",
    "        \n",
    "        feature_importance = self.metadata.get('feature_importance', [])\n",
    "        \n",
    "        if not feature_importance:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        lines = []\n",
    "        for i, feat in enumerate(feature_importance[:10], 1):\n",
    "            if isinstance(feat, dict):\n",
    "                lines.append(f\"{i}. **{feat.get('feature', 'N/A')}**: {feat.get('importance', 0):.4f}\")\n",
    "            else:\n",
    "                lines.append(f\"{i}. {feat}\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, artifact_dir: Path) -> 'ModelArtifact':\n",
    "        \"\"\"Carrega artefato salvo.\"\"\"\n",
    "        \n",
    "        logger.info(f\"Carregando artefato de: {artifact_dir}\")\n",
    "        \n",
    "        # Carrega modelo\n",
    "        model_path = artifact_dir / 'model.joblib'\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Carrega metadados\n",
    "        metadata_path = artifact_dir / 'metadata.json'\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Carrega configura\u00e7\u00e3o\n",
    "        config_path = artifact_dir / 'config.json'\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        \n",
    "        # Reconstr\u00f3i config (simplificado)\n",
    "        config = EnterpriseConfig()\n",
    "        \n",
    "        # Carrega explainer se existir\n",
    "        explainer_path = artifact_dir / 'explainer.joblib'\n",
    "        explainer = None\n",
    "        if explainer_path.exists():\n",
    "            explainer = joblib.load(explainer_path)\n",
    "        \n",
    "        logger.info(\"\u2705 Artefato carregado com sucesso\")\n",
    "        \n",
    "        return cls(model, metadata, config, explainer)\n",
    "    \n",
    "    def validate(self, X_sample: pd.DataFrame) -> bool:\n",
    "        \"\"\"Valida integridade do modelo.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Testa predi\u00e7\u00e3o\n",
    "            predictions = self.model.predict_proba(X_sample)\n",
    "            \n",
    "            # Valida\u00e7\u00f5es\n",
    "            assert predictions.shape[1] == 2, \"Modelo deve ter 2 classes\"\n",
    "            assert np.all((predictions >= 0) & (predictions <= 1)), \"Probabilidades devem estar em [0, 1]\"\n",
    "            assert not np.any(np.isnan(predictions)), \"Predi\u00e7\u00f5es n\u00e3o podem conter NaN\"\n",
    "            \n",
    "            # Verifica hash\n",
    "            current_hash = self._compute_hash()\n",
    "            if current_hash != self.model_hash:\n",
    "                logger.warning(\"\u26a0\ufe0f  Hash do modelo mudou! Poss\u00edvel corrup\u00e7\u00e3o.\")\n",
    "                return False\n",
    "            \n",
    "            logger.info(\"\u2705 Valida\u00e7\u00e3o do modelo: PASSOU\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"\u274c Valida\u00e7\u00e3o falhou: {e}\")\n",
    "            return False\n",
    "\n",
    "# ==============================================================================\n",
    "# 13. PIPELINE PRINCIPAL DE TREINO\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# CORRE\u00c7\u00c3O FINAL: Adicionar remo\u00e7\u00e3o de colunas no _load_data()\n",
    "# ==============================================================================\n",
    "\n",
    "class EnterpriseCreditScoringPipeline:\n",
    "    \"\"\"Pipeline completo enterprise para Credit Scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[EnterpriseConfig] = None):\n",
    "        self.config = config or EnterpriseConfig()\n",
    "        self.validator = DataValidator(self.config)\n",
    "        self.splitter = TemporalDataSplitter(self.config.ml_config)\n",
    "        self.optimizer = BayesianOptimizer(self.config)\n",
    "        self.cutoff_optimizer = CutoffOptimizer(self.config.business_metrics)\n",
    "        \n",
    "        self.preprocessing_pipeline = None\n",
    "        self.model = None\n",
    "        self.model_artifact = None\n",
    "        self.explainer = None\n",
    "        \n",
    "        self.results = {\n",
    "            'train_metrics': {},\n",
    "            'validation_metrics': {},\n",
    "            'test_metrics': {},\n",
    "            'oot_metrics': {},\n",
    "            'feature_importance': [],\n",
    "            'optimization_history': None\n",
    "        }\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Executa pipeline completo.\"\"\"\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*70)\n",
    "        logger.info(\"\ud83d\ude80 INICIANDO PIPELINE ENTERPRISE DE CREDIT SCORING\")\n",
    "        logger.info(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Carregamento e LIMPEZA de Dados\n",
    "            logger.info(\"ETAPA 1/10: Carregamento e Limpeza de Dados\")\n",
    "            logger.info(\"-\" * 70)\n",
    "            df = self._load_and_clean_data()  # \u2190 MODIFICADO\n",
    "            \n",
    "            # 2. Valida\u00e7\u00e3o (agora sem colunas de leakage)\n",
    "            logger.info(\"\\nETAPA 2/10: Valida\u00e7\u00e3o de Qualidade\")\n",
    "            logger.info(\"-\" * 70)\n",
    "            is_valid, validation_results = self.validator.validate_all(df)\n",
    "            \n",
    "            if not is_valid:\n",
    "                raise ValueError(\"Dados falharam na valida\u00e7\u00e3o. Verifique os logs.\")\n",
    "            \n",
    "            # 3. Prepara\u00e7\u00e3o de Dados\n",
    "            logger.info(\"\\nETAPA 3/10: Prepara\u00e7\u00e3o de Dados\")\n",
    "            logger.info(\"-\" * 70)\n",
    "            X, y = self._prepare_data(df)\n",
    "            \n",
    "            # ... resto do c\u00f3digo permanece igual\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"\\n\u274c ERRO FATAL NO PIPELINE: {e}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    def _load_and_clean_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Carrega dados E REMOVE colunas de leakage.\"\"\"\n",
    "        \n",
    "        data_path = self.config.ml_config.data_path\n",
    "        sheet_name = self.config.ml_config.sheet_name\n",
    "        \n",
    "        logger.info(f\"Carregando dados de: {data_path}\")\n",
    "        df = pd.read_excel(data_path, sheet_name=sheet_name)\n",
    "        logger.info(f\"\u2705 Dados carregados: {df.shape[0]} registros, {df.shape[1]} colunas\")\n",
    "        \n",
    "        # REMOVE COLUNAS DE LEAKAGE IMEDIATAMENTE\n",
    "        cols_to_remove = [\n",
    "            'saldo_vencido', \n",
    "            'quantidade_parcelas_vencidas',\n",
    "            'primeiro_vencimento_em_atraso',\n",
    "            'dias_em_atraso',\n",
    "            'data_quitacao',\n",
    "            'taxa_parcelas_vencidas',\n",
    "            'recebido',\n",
    "            'produtor',\n",
    "            'lancamento',\n",
    "            'pedido_id'\n",
    "        ]\n",
    "        \n",
    "        cols_found = [col for col in cols_to_remove if col in df.columns]\n",
    "        \n",
    "        if cols_found:\n",
    "            logger.info(f\"\ud83e\uddf9 Removendo {len(cols_found)} colunas de leakage/desnecess\u00e1rias:\")\n",
    "            for col in cols_found:\n",
    "                logger.info(f\"   - {col}\")\n",
    "            df = df.drop(columns=cols_found)\n",
    "            logger.info(f\"\u2705 Dados limpos: {df.shape[1]} colunas restantes\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Prepara dados para modelagem.\"\"\"\n",
    "        \n",
    "        # Target\n",
    "        target_col = self.config.ml_config.target_variable\n",
    "        \n",
    "        if target_col not in df.columns:\n",
    "            raise ValueError(f\"Coluna target '{target_col}' n\u00e3o encontrada\")\n",
    "        \n",
    "        # Binariza target\n",
    "        y = df[target_col].apply(lambda x: 1 if str(x).lower() in ['inadimplente', '1', 'true'] else 0)\n",
    "        X = df.drop(columns=[target_col])\n",
    "        \n",
    "        logger.info(f\"\u2705 Dados preparados:\")\n",
    "        logger.info(f\"   Features: {X.shape[1]}\")\n",
    "        logger.info(f\"   Target distribution:\")\n",
    "        logger.info(f\"      Bom Pagador (0): {(y==0).sum()} ({(y==0).mean()*100:.2f}%)\")\n",
    "        logger.info(f\"      Inadimplente (1): {(y==1).sum()} ({(y==1).mean()*100:.2f}%)\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def _evaluate_all_sets(self, X_train, y_train, X_val, y_val, X_test, y_test, X_oot, y_oot):\n",
    "        \"\"\"Avalia modelo em todos os conjuntos.\"\"\"\n",
    "        \n",
    "        sets = {\n",
    "            'train': (X_train, y_train),\n",
    "            'validation': (X_val, y_val),\n",
    "            'test': (X_test, y_test),\n",
    "            'oot': (X_oot, y_oot)\n",
    "        }\n",
    "        \n",
    "        for set_name, (X, y) in sets.items():\n",
    "            logger.info(f\"\\n  Avaliando {set_name.upper()}...\")\n",
    "            \n",
    "            y_prob = self.model.predict_proba(X)[:, 1]\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "            \n",
    "            metrics = CreditScoringMetrics.calculate_all_metrics(\n",
    "                y.values, y_prob, y_pred\n",
    "            )\n",
    "            \n",
    "            business_value = CreditScoringMetrics.calculate_business_value(\n",
    "                metrics,\n",
    "                self.config.business_metrics\n",
    "            )\n",
    "            \n",
    "            metrics.update(business_value)\n",
    "            \n",
    "            self.results[f'{set_name}_metrics'] = metrics\n",
    "            \n",
    "            # Log principais m\u00e9tricas\n",
    "            logger.info(f\"    AUC: {metrics['auc']:.4f}\")\n",
    "            logger.info(f\"    Gini: {metrics['gini']:.4f}\")\n",
    "            logger.info(f\"    KS: {metrics['ks_statistic']:.4f}\")\n",
    "            logger.info(f\"    Bad Rate: {metrics['bad_rate']:.2%}\")\n",
    "            logger.info(f\"    Net Profit: R$ {metrics['net_profit']:,.2f}\")\n",
    "    \n",
    "    def _save_model_artifact(self):\n",
    "        \"\"\"Salva artefato completo do modelo.\"\"\"\n",
    "        \n",
    "        metadata = {\n",
    "            'model_type': 'Calibrated Ensemble',\n",
    "            'train_metrics': self.results['train_metrics'],\n",
    "            'validation_metrics': self.results['validation_metrics'],\n",
    "            'test_metrics': self.results['test_metrics'],\n",
    "            'oot_metrics': self.results['oot_metrics'],\n",
    "            'optimal_cutoff': self.results['optimal_cutoff'],\n",
    "            'cutoff_strategy': self.results['cutoff_strategy'],\n",
    "            'feature_importance': self.results['feature_importance'][:20],\n",
    "            'n_features': len(self.results.get('feature_importance', [])),\n",
    "            'leakage_check': 'PASSED'\n",
    "        }\n",
    "        \n",
    "        self.model_artifact = ModelArtifact(\n",
    "            model=self.model,\n",
    "            metadata=metadata,\n",
    "            config=self.config,\n",
    "            explainer=self.explainer\n",
    "        )\n",
    "        \n",
    "        artifact_dir = self.model_artifact.save()\n",
    "        \n",
    "        logger.info(f\"\\n\u2705 Artefato completo salvo em: {artifact_dir}\")\n",
    "    \n",
    "    def _print_final_summary(self):\n",
    "        \"\"\"Imprime sum\u00e1rio final.\"\"\"\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*70)\n",
    "        logger.info(\"\ud83d\udcca SUM\u00c1RIO FINAL DE PERFORMANCE\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        for set_name in ['train', 'validation', 'test', 'oot']:\n",
    "            metrics = self.results.get(f'{set_name}_metrics', {})\n",
    "            \n",
    "            if metrics:\n",
    "                logger.info(f\"\\n{set_name.upper()}:\")\n",
    "                logger.info(f\"  AUC: {metrics.get('auc', 0):.4f}\")\n",
    "                logger.info(f\"  Gini: {metrics.get('gini', 0):.4f}\")\n",
    "                logger.info(f\"  KS: {metrics.get('ks_statistic', 0):.4f}\")\n",
    "                logger.info(f\"  Approval Rate: {metrics.get('approval_rate', 0):.2%}\")\n",
    "                logger.info(f\"  Bad Rate: {metrics.get('bad_rate', 0):.2%}\")\n",
    "                logger.info(f\"  Net Profit: R$ {metrics.get('net_profit', 0):,.2f}\")\n",
    "        \n",
    "        logger.info(f\"\\nCUTOFF \u00d3TIMO: {self.results.get('optimal_cutoff', 0):.4f}\")\n",
    "        logger.info(\"=\"*70)\n",
    "\n",
    "# ==============================================================================\n",
    "# 14. PONTO DE ENTRADA\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun\u00e7\u00e3o principal.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Cria configura\u00e7\u00e3o\n",
    "        config = EnterpriseConfig()\n",
    "        \n",
    "        # Customiza configura\u00e7\u00e3o se necess\u00e1rio\n",
    "        config.ml_config.n_trials_optuna = 50  # Reduz para teste mais r\u00e1pido\n",
    "        \n",
    "        # Cria e executa pipeline\n",
    "        pipeline = EnterpriseCreditScoringPipeline(config)\n",
    "        pipeline.run()\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"\u274c Arquivo de dados n\u00e3o encontrado: {e}\")\n",
    "        logger.error(\"   Verifique o caminho em EnterpriseConfig.ml_config.data_path\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\u274c Erro fatal: {e}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# ==============================================================================\n",
    "# FIM DO C\u00d3DIGO ENTERPRISE 10/10\n",
    "# =============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}